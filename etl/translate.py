#!/usr/bin/env python3
"""
Polymarket ì‹œì¥ ì œëª© í•œê¸€ ë²ˆì—­ (í†µí•© ìŠ¤í¬ë¦½íŠ¸)

ì‚¬ìš©ë²•:
    # ê¸°ë³¸: ì•ìœ¼ë¡œ 2ê°œì›”, ë¯¸ë²ˆì—­ë§Œ
    python translate.py

    # Sports ì œì™¸, 6ê°œì›”, 5ì›Œì»¤
    python translate.py --exclude-sports --months 6 --workers 5

    # ë®ì–´ì“°ê¸° ëª¨ë“œ (ì´ë¯¸ ë²ˆì—­ëœ ê²ƒë„ ì¬ë²ˆì—­)
    python translate.py --overwrite

    # íŠ¹ì • ë‚ ì§œ ë²”ìœ„
    python translate.py --from 2026-02-11 --to 2026-04-11

    # í…ŒìŠ¤íŠ¸ (1ë°°ì¹˜ë§Œ)
    python translate.py --test
"""

import os
import sys
import time
import queue
import threading
import argparse
from typing import List, Dict
from pathlib import Path
from datetime import datetime, timedelta, timezone
from concurrent.futures import ThreadPoolExecutor, as_completed
from dotenv import load_dotenv
from openai import OpenAI
from supabase import create_client, Client
from postprocess import postprocess_translation

# .env ë¡œë“œ
env_path = Path(__file__).parent.parent / '.env'
load_dotenv(dotenv_path=env_path)

# ì„¤ì •ê°’
TRANSLATE_BATCH_SIZE = 100   # OpenAI API ë°°ì¹˜ í¬ê¸°
UPSERT_BATCH_SIZE = 500      # DB upsert ë°°ì¹˜ í¬ê¸°
CACHE_QUERY_SIZE = 200       # ìºì‹œ ì¡°íšŒ ì²­í¬ í¬ê¸°
MAX_RETRIES = 3


def load_translation_prompt() -> str:
    """translation_prompt.mdì—ì„œ í”„ë¡¬í”„íŠ¸ ë¡œë“œ"""
    prompt_file = Path(__file__).parent / 'translation_prompt.md'
    try:
        content = prompt_file.read_text(encoding='utf-8')
        start = content.find('```\në‹¹ì‹ ì€') + 4
        end = content.find('\n```', start)
        if start > 3 and end > start:
            return content[start:end].strip()
    except Exception as e:
        print(f"âš ï¸  í”„ë¡¬í”„íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}")

    return """ë‹¹ì‹ ì€ Polymarket ì˜ˆì¸¡ ì‹œì¥ ì œëª©ì„ í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ë°˜ë§ë¡œ ë²ˆì—­í•˜ì„¸ìš” (~í• ê¹Œ?, ~ë ê¹Œ?). ë‚ ì§œëŠ” í•œê¸€ë¡œ (February 11 â†’ 2ì›” 11ì¼).
ì‹œê°„ëŒ€ëŠ” ë°˜ë“œì‹œ ìœ ì§€ (4AM ET â†’ ì˜¤ì „ 4ì‹œ ET). ë²ˆí˜¸ì™€ í•¨ê»˜ ì¶œë ¥í•˜ì„¸ìš”."""


TRANSLATION_PROMPT = load_translation_prompt()


def calculate_date_range(months: int, from_date: str = None, to_date: str = None):
    """ë‚ ì§œ ë²”ìœ„ ê³„ì‚° (KST ê¸°ì¤€)"""
    if from_date and to_date:
        return from_date, to_date

    now_utc = datetime.now(timezone.utc)
    # KST ì˜¤ëŠ˜ ì‹œì‘ = UTC ì „ë‚  15:00
    today_start = now_utc.replace(hour=15, minute=0, second=0, microsecond=0) - timedelta(days=1)
    if now_utc.hour >= 15:
        today_start = now_utc.replace(hour=15, minute=0, second=0, microsecond=0)

    start = today_start.strftime('%Y-%m-%d %H:%M:%S+00')
    end = (today_start + timedelta(days=months * 30)).strftime('%Y-%m-%d %H:%M:%S+00')
    return start, end


class Translator:
    def __init__(self, workers: int, overwrite: bool, exclude_sports: bool,
                 start_date: str, end_date: str):
        # í™˜ê²½ ë³€ìˆ˜
        self.openai_key = os.getenv('OPENAI_API_KEY')
        self.supabase_url = os.getenv('SUPABASE_URL')
        self.supabase_key = os.getenv('SUPABASE_KEY')

        if not all([self.openai_key, self.supabase_url, self.supabase_key]):
            print("âŒ í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”:")
            print("   OPENAI_API_KEY, SUPABASE_URL, SUPABASE_KEY")
            sys.exit(1)

        # í´ë¼ì´ì–¸íŠ¸
        self.openai_client = OpenAI(api_key=self.openai_key)
        self.supabase: Client = create_client(self.supabase_url, self.supabase_key)

        # ì˜µì…˜
        self.workers = workers
        self.overwrite = overwrite
        self.exclude_sports = exclude_sports
        self.start_date = start_date
        self.end_date = end_date

        # Supabase í´ë¼ì´ì–¸íŠ¸ í’€ (ì›Œì»¤ìš©)
        self.client_pool = queue.Queue()
        for _ in range(workers):
            self.client_pool.put(create_client(self.supabase_url, self.supabase_key))

        # system ë©”ì‹œì§€ì— TRANSLATION_PROMPT í†µí•© (í† í° ë¹„ìš© ì ˆê°)
        self.system_message = f"""{TRANSLATION_PROMPT}

---
ì¶”ê°€ ê·œì¹™:
1. ë°˜ë“œì‹œ ë°˜ë§ë¡œ ë²ˆì—­ (~í• ê¹Œ, ~ë ê¹Œ, ~ì¸ê°€)
2. ì ˆëŒ€ ì¡´ëŒ“ë§ ì‚¬ìš© ê¸ˆì§€ (~í• ê¹Œìš”, ~ë ê¹Œìš” âŒ)
3. ì‹œê°„ëŒ€ í‘œê¸° í•„ìˆ˜: ET, PT ë“±ì€ ë°˜ë“œì‹œ ìœ ì§€ (4AM ET â†’ ì˜¤ì „ 4ì‹œ ET âœ…)
4. "have"ë¥¼ "ê°€ì§€ë‹¤"ë¡œ ì§ì—­ ê¸ˆì§€. ë¬¸ë§¥ì— ë§ê²Œ "ì°¨ì§€í• ê¹Œ/ì„ ë³´ì¼ê¹Œ/ê¸°ë¡í• ê¹Œ" ì‚¬ìš©
5. ëª¨ë“  ì œëª©ì—ì„œ ì¼ê´€ì„± ìœ ì§€"""

        # í†µê³„ (Thread-safe)
        self.lock = threading.Lock()
        self.total_translated = 0
        self.total_api_calls = 0
        self.failed_batches = 0
        self.cache_hits = 0

    def _get_client(self) -> Client:
        """í’€ì—ì„œ Supabase í´ë¼ì´ì–¸íŠ¸ ê°€ì ¸ì˜¤ê¸°"""
        return self.client_pool.get()

    def _return_client(self, client: Client):
        """í’€ì— Supabase í´ë¼ì´ì–¸íŠ¸ ë°˜í™˜"""
        self.client_pool.put(client)

    def translate_batch(self, titles: List[str]) -> Dict[str, str]:
        """OpenAI APIë¡œ ë°°ì¹˜ ë²ˆì—­, titleâ†’title_ko ë§¤í•‘ ë°˜í™˜"""
        if not titles:
            return {}

        titles_text = "\n".join([f"{i+1}. {t}" for i, t in enumerate(titles)])

        for attempt in range(MAX_RETRIES):
            try:
                completion = self.openai_client.chat.completions.create(
                    model="gpt-4o-mini",
                    max_tokens=5000,
                    temperature=0.3,
                    messages=[
                        {"role": "system", "content": self.system_message},
                        {"role": "user", "content": f"ë²ˆì—­í•  ì œëª©ë“¤:\n{titles_text}"}
                    ]
                )

                response_text = completion.choices[0].message.content.strip()

                # ë²ˆí˜¸ ê¸°ë°˜ íŒŒì‹±
                translations_dict = {}
                for line in response_text.split('\n'):
                    line = line.strip()
                    if not line:
                        continue
                    if '. ' in line and line[0].isdigit():
                        parts = line.split('. ', 1)
                        try:
                            num = int(parts[0])
                            translations_dict[num] = parts[1]
                        except (ValueError, IndexError):
                            continue

                # í›„ì²˜ë¦¬ + titleâ†’title_ko ë§¤í•‘ ìƒì„±
                result = {}
                for i, title in enumerate(titles):
                    translation = translations_dict.get(i + 1, title)
                    translation = postprocess_translation(title, translation)
                    result[title] = translation

                if len(result) != len(titles):
                    print(f"  âš ï¸  ë²ˆì—­ ê°œìˆ˜ ë¶ˆì¼ì¹˜: {len(result)} != {len(titles)}")

                return result

            except Exception as e:
                if attempt < MAX_RETRIES - 1:
                    print(f"  âš ï¸  ì¬ì‹œë„ {attempt + 1}/{MAX_RETRIES}")
                    time.sleep(2 ** attempt)
                else:
                    print(f"  âŒ API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
                    return {}

        return {}

    def _preload_cache(self, titles: List[str]) -> Dict[str, str]:
        """ì „ì²´ ëŒ€ìƒ titleì— ëŒ€í•´ ê¸°ì¡´ ë²ˆì—­ ìºì‹œë¥¼ í•œë²ˆì— ì¡°íšŒ"""
        cache = {}
        unique_titles = list(set(titles))

        print(f"  ìºì‹œ ì¡°íšŒ ì¤‘... ({len(unique_titles):,}ê°œ ê³ ìœ  ì œëª©)")

        for i in range(0, len(unique_titles), CACHE_QUERY_SIZE):
            chunk = unique_titles[i:i + CACHE_QUERY_SIZE]
            try:
                response = self.supabase.table('poly_events') \
                    .select('title, title_ko') \
                    .in_('title', chunk) \
                    .not_.is_('title_ko', 'null') \
                    .execute()
                for row in response.data:
                    if row['title'] not in cache:
                        cache[row['title']] = row['title_ko']
            except Exception as e:
                print(f"  âš ï¸  ìºì‹œ ì¡°íšŒ ì‹¤íŒ¨ (ì²­í¬ {i//CACHE_QUERY_SIZE + 1}): {e}")

        print(f"  ìºì‹œ ì ì¤‘  : {len(cache):,}ê°œ")
        return cache

    def _bulk_upsert(self, events: List[Dict], title_map: Dict[str, str]) -> int:
        """title_mapì„ ê¸°ë°˜ìœ¼ë¡œ ì „ì²´ ì´ë²¤íŠ¸ì— title_koë¥¼ ë²Œí¬ upsert"""
        upsert_data = []
        for event in events:
            title_ko = title_map.get(event['title'])
            if title_ko:
                upsert_data.append({'id': event['id'], 'title_ko': title_ko})

        if not upsert_data:
            return 0

        success = 0
        total_chunks = (len(upsert_data) + UPSERT_BATCH_SIZE - 1) // UPSERT_BATCH_SIZE

        for i in range(0, len(upsert_data), UPSERT_BATCH_SIZE):
            chunk = upsert_data[i:i + UPSERT_BATCH_SIZE]
            chunk_num = i // UPSERT_BATCH_SIZE + 1

            for attempt in range(MAX_RETRIES):
                try:
                    result = self.supabase.table('poly_events') \
                        .upsert(chunk, on_conflict='id') \
                        .execute()
                    success += len(result.data)
                    print(f"  ğŸ’¾ DB ì €ì¥ {chunk_num}/{total_chunks} | {len(result.data)}ê°œ")
                    break
                except Exception as e:
                    if attempt < MAX_RETRIES - 1:
                        time.sleep(1 * (attempt + 1))
                    else:
                        print(f"  âŒ DB ì €ì¥ ì‹¤íŒ¨ (ì²­í¬ {chunk_num}): {e}")

        return success

    def _translate_batch_worker(self, batch_num: int, titles: List[str],
                                total_batches: int) -> Dict[str, str]:
        """ì›Œì»¤ ìŠ¤ë ˆë“œì—ì„œ ë°°ì¹˜ ë²ˆì—­ ì‹¤í–‰"""
        try:
            result = self.translate_batch(titles)

            with self.lock:
                self.total_api_calls += 1
                translated_count = len(result)

            progress = (self.total_api_calls / total_batches) * 100
            print(f"  ğŸ”¤ ë²ˆì—­ {batch_num:3d}/{total_batches} | "
                  f"{translated_count:3d}ê°œ ì™„ë£Œ ({progress:.1f}%)")

            return result

        except Exception as e:
            with self.lock:
                self.failed_batches += 1
            print(f"  âŒ ë²ˆì—­ ë°°ì¹˜ {batch_num} ì‹¤íŒ¨: {e}")
            return {}

    def fetch_all_target_ids(self) -> List[Dict]:
        """ë²ˆì—­ ëŒ€ìƒ ì´ë²¤íŠ¸ì˜ id, titleì„ í•œë²ˆì— ëª¨ë‘ ì¡°íšŒ"""
        all_events = []
        offset = 0
        page_size = 1000

        while True:
            query = self.supabase.table('poly_events') \
                .select('id, title') \
                .gte('end_date', self.start_date) \
                .lt('end_date', self.end_date)

            if not self.overwrite:
                query = query.is_('title_ko', 'null')

            if self.exclude_sports:
                query = query.neq('category', 'Sports')

            response = query.order('end_date').limit(page_size).offset(offset).execute()

            if not response.data:
                break

            all_events.extend(response.data)
            offset += page_size

            if len(response.data) < page_size:
                break

        return all_events

    def run(self, max_batches: int = None):
        """ë²ˆì—­ ì‹¤í–‰"""
        # ì„¤ì • ì¶œë ¥
        print(f"\n{'='*55}")
        print(f"  Polymarket ì œëª© ë²ˆì—­")
        print(f"{'='*55}")
        print(f"  ê¸°ê°„       : {self.start_date[:10]} ~ {self.end_date[:10]}")
        print(f"  ì›Œì»¤       : {self.workers}ê°œ")
        print(f"  ëª¨ë“œ       : {'ë®ì–´ì“°ê¸°' if self.overwrite else 'ë¯¸ë²ˆì—­ë§Œ'}")
        if self.exclude_sports:
            print(f"  ì œì™¸       : Sports")
        print()

        # 1. ëŒ€ìƒ ì´ë²¤íŠ¸ ì „ì²´ ì¡°íšŒ
        print("  ì´ë²¤íŠ¸ ì¡°íšŒ ì¤‘...")
        all_events = self.fetch_all_target_ids()
        total_events = len(all_events)

        if total_events == 0:
            print("  âœ… ë²ˆì—­í•  ì´ë²¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.\n")
            return

        # 2. ê³ ìœ  ì œëª© ì¶”ì¶œ (ì¤‘ë³µ ì œê±°)
        all_titles = [e['title'] for e in all_events]
        unique_titles = list(set(all_titles))
        dedup_saved = total_events - len(unique_titles)

        # 3. ìºì‹œ ì„ ë¡œë”© (ë®ì–´ì“°ê¸° ëª¨ë“œê°€ ì•„ë‹ ë•Œë§Œ)
        cache = {}
        if not self.overwrite:
            cache = self._preload_cache(unique_titles)
            self.cache_hits = len(cache)

        # 4. ë²ˆì—­ í•„ìš”í•œ ì œëª©ë§Œ í•„í„°
        titles_to_translate = [t for t in unique_titles if t not in cache]

        # 5. ë²ˆì—­ ë°°ì¹˜ ë¶„í• 
        translate_batches = [
            titles_to_translate[i:i + TRANSLATE_BATCH_SIZE]
            for i in range(0, len(titles_to_translate), TRANSLATE_BATCH_SIZE)
        ]
        total_translate_batches = len(translate_batches)

        if max_batches and total_translate_batches > max_batches:
            translate_batches = translate_batches[:max_batches]
            total_translate_batches = len(translate_batches)

        print(f"\n  ëŒ€ìƒ ì´ë²¤íŠ¸ : {total_events:,}ê°œ")
        print(f"  ê³ ìœ  ì œëª©   : {len(unique_titles):,}ê°œ (ì¤‘ë³µ {dedup_saved:,}ê°œ ì œê±°)")
        if not self.overwrite:
            print(f"  ìºì‹œ ì ì¤‘   : {len(cache):,}ê°œ")
        print(f"  ë²ˆì—­ í•„ìš”   : {len(titles_to_translate):,}ê°œ")
        print(f"  ë²ˆì—­ ë°°ì¹˜   : {total_translate_batches}ê°œ")
        if total_translate_batches > 0:
            print(f"  ì˜ˆìƒ ì‹œê°„   : ~{(total_translate_batches * 1.5 / self.workers / 60):.1f}ë¶„")
        print(f"{'='*55}\n")

        start_time = time.time()

        # 6. ë³‘ë ¬ ë²ˆì—­ (unique title ê¸°ì¤€)
        title_map = dict(cache)  # ìºì‹œ ê²°ê³¼ë¥¼ ë¨¼ì € í¬í•¨

        if translate_batches:
            print("  [ë²ˆì—­ ë‹¨ê³„]")
            with ThreadPoolExecutor(max_workers=self.workers) as executor:
                futures = {
                    executor.submit(
                        self._translate_batch_worker, i + 1, batch, total_translate_batches
                    ): i + 1
                    for i, batch in enumerate(translate_batches)
                }
                for future in as_completed(futures):
                    batch_result = future.result()
                    title_map.update(batch_result)

        # 7. ë²Œí¬ DB ì—…ë°ì´íŠ¸ (max_batches ì ìš© ì‹œ ë²ˆì—­ëœ ì œëª©ë§Œ í•„í„°)
        if max_batches:
            translated_titles = set(title_map.keys())
            events_to_update = [e for e in all_events if e['title'] in translated_titles]
        else:
            events_to_update = all_events

        print(f"\n  [DB ì €ì¥ ë‹¨ê³„]")
        self.total_translated = self._bulk_upsert(events_to_update, title_map)

        # 8. ê²°ê³¼ ì¶œë ¥
        elapsed = time.time() - start_time
        print(f"\n{'='*55}")
        print(f"  ë²ˆì—­ ì™„ë£Œ!")
        print(f"  ì´ë²¤íŠ¸ ì—…ë°ì´íŠ¸ : {self.total_translated:,}ê°œ")
        print(f"  ê³ ìœ  ë²ˆì—­       : {len(title_map):,}ê°œ")
        if self.cache_hits > 0:
            print(f"  ìºì‹œ ì¬ì‚¬ìš©     : {self.cache_hits:,}ê°œ")
        if dedup_saved > 0:
            print(f"  ì¤‘ë³µ ì ˆê°       : {dedup_saved:,}ê°œ (API í˜¸ì¶œ ì ˆì•½)")
        print(f"  ì‹¤íŒ¨ ë°°ì¹˜       : {self.failed_batches}ê°œ")
        print(f"  ì‹œê°„            : {elapsed/60:.1f}ë¶„")
        if self.total_translated > 0:
            print(f"  ì†ë„            : {self.total_translated/(elapsed/60):.0f}ê°œ/ë¶„")
        print(f"{'='*55}\n")


def main():
    parser = argparse.ArgumentParser(
        description='Polymarket ì œëª© í•œê¸€ ë²ˆì—­',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì‹œ:
  python translate.py                              # ê¸°ë³¸ (2ê°œì›”, ë¯¸ë²ˆì—­ë§Œ)
  python translate.py --exclude-sports -m 6 -w 5   # Sports ì œì™¸, 6ê°œì›”, 5ì›Œì»¤
  python translate.py --overwrite -m 2             # 2ê°œì›” ì „ì²´ ì¬ë²ˆì—­
  python translate.py --from 2026-02-11 --to 2026-04-11  # ë‚ ì§œ ì§€ì •
  python translate.py --test                       # í…ŒìŠ¤íŠ¸ (1ë°°ì¹˜)
        """)

    parser.add_argument('-w', '--workers', type=int, default=4,
                        help='ì›Œì»¤ ìˆ˜ (ê¸°ë³¸: 4, ê¶Œì¥: 3-5)')
    parser.add_argument('-m', '--months', type=int, default=2,
                        help='ë²ˆì—­ ê¸°ê°„ - ì˜¤ëŠ˜ë¶€í„° Nê°œì›” (ê¸°ë³¸: 2)')
    parser.add_argument('--from', dest='from_date', type=str, default=None,
                        help='ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD)')
    parser.add_argument('--to', dest='to_date', type=str, default=None,
                        help='ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD)')
    parser.add_argument('--overwrite', action='store_true',
                        help='ì´ë¯¸ ë²ˆì—­ëœ ê²ƒë„ ì¬ë²ˆì—­')
    parser.add_argument('--exclude-sports', action='store_true',
                        help='Sports ì¹´í…Œê³ ë¦¬ ì œì™¸')
    parser.add_argument('--max-batches', type=int, default=None,
                        help='ìµœëŒ€ ë°°ì¹˜ ìˆ˜ (í…ŒìŠ¤íŠ¸ìš©)')
    parser.add_argument('--test', action='store_true',
                        help='í…ŒìŠ¤íŠ¸ ëª¨ë“œ (1ë°°ì¹˜ë§Œ)')

    args = parser.parse_args()

    if args.test:
        args.max_batches = 1

    if args.workers > 10:
        print("âš ï¸  ì›Œì»¤ê°€ ë„ˆë¬´ ë§ìœ¼ë©´ API Rate Limitì— ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤ (ê¶Œì¥: 3-5)")
        if input("   ê³„ì†? (y/N): ").lower() != 'y':
            sys.exit(0)

    start_date, end_date = calculate_date_range(
        args.months, args.from_date, args.to_date
    )

    translator = Translator(
        workers=args.workers,
        overwrite=args.overwrite,
        exclude_sports=args.exclude_sports,
        start_date=start_date,
        end_date=end_date,
    )
    translator.run(max_batches=args.max_batches)


if __name__ == '__main__':
    main()
